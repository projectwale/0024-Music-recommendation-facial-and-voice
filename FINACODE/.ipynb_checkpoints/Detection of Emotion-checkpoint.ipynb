{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e9746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360017c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emotion_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6908/3614369295.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m#emotion_model.load_weights('emotion_model.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m#emotion_model.load_weights('')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0memotion_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memotion_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;31m#print(emotion_prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mmaxindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emotion_model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "\n",
    "\n",
    "#resize OPENCV window size\n",
    "#cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# load model architecture and weights\n",
    "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\",\"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "# initialize camera stream\n",
    "vid=cv2.VideoCapture(0)\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read frame from camer and resize to 400 pixels\n",
    "    ret,frame = vid.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    " \n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
    " \n",
    "    # pass the blob through the network and obtain the detections and\n",
    "    # predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    #print(detections)\n",
    "    faceBoxes = []\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out detections by confidence\n",
    "        if confidence < 0.7:\n",
    "            continue\n",
    "\n",
    "        # compute the (x, y)-coordinates of the bounding box for the\n",
    "        # object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #print(gray_frame)\n",
    "        #print(box)\n",
    "        frameHeight = frame.shape[0]\n",
    "        frameWidth = frame.shape[1]\n",
    "        x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "        y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "        x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "        y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "        faceBoxes.append([x1, y1, x2, y2])\n",
    "        #faceBoxes=box\n",
    "        #print(faceBoxes)\n",
    "        #print(faceBoxes[0][2])\n",
    "        \n",
    "        roi_gray_frame = gray_frame[faceBoxes[0][2]:faceBoxes[0][2] + faceBoxes[0][3], faceBoxes[0][0]:faceBoxes[0][0] + faceBoxes[0][3]]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "        emotionlistforadding=['Angry','Disgusted','Fearful','Happy','Neutral','Sad','Surprised']\n",
    "        #break\n",
    "        import tensorflow as tf\n",
    "        new_model = tf.keras.models.load_model('NewEmotion.hp5')\n",
    "        #emotion_model.load_weights('emotion_model.h5')\n",
    "        #emotion_model.load_weights('')\n",
    "        emotion_prediction = new_model.predict(cropped_img)\n",
    "        #print(emotion_prediction)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        print(maxindex)\n",
    "        labelemotion= \"{}\".format(\"Emotion : \" + emotion_dict[maxindex])\n",
    "        \n",
    "         \n",
    "        # draw the bounding box of the face along with the associated\n",
    "        # probability\n",
    "        text = \"{:.2f}%\".format(confidence * 100)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY),(0, 0, 255), 2)\n",
    "        cv2.putText(frame, labelemotion, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
    "                    (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        #cv2.putText(frame, text, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    \n",
    "\n",
    "\t# show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab983c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
